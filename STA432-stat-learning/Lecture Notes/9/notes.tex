\documentclass[titlepage, 12pt, leqno]{article}

% -------------------------------------------------- %
% -------------------- PACKAGES -------------------- %
% -------------------------------------------------- %
\usepackage{import}
\usepackage{pdfpages}
\usepackage{mathtools}
\usepackage{transparent}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{parskip}
\usepackage{bbm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algpseudocodex}
\usepackage[margin = 1in]{geometry}
\tcbuselibrary{breakable}
\tcbset{breakable = true}


% -------------------------------------------------- %
% -------------- CUSTOM ENVIRONMENTS --------------- %
% -------------------------------------------------- %
\newtcolorbox{note}{colback=black!5!white,
                          colframe=black!55!white,
                          fonttitle=\bfseries,title=Note}

\newtcolorbox{ex}{colback=blue!5!white,
                          colframe=blue!55!white,
                          fonttitle=\bfseries,title=Example}

\newtcolorbox{definition}{colback=red!5!white,
                          colframe=red!55!white,
                          fonttitle=\bfseries,title=Definition}


% -------------------------------------------------- %
% ------------------- COMMANDS --------------------- %
% -------------------------------------------------- %
% Brackets, braces, etc. 
\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\bigabs}[1]{\Bigl \lvert #1 \Bigr \rvert}
\newcommand{\bigbracket}[1]{\Bigl [ #1 \Bigr ]}
\newcommand{\bigparen}[1]{\Bigl ( #1 \Bigr )}
\newcommand{\ceil}[1]{\lceil #1 \rceil}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\norm}[1]{\| #1 \|}
\newcommand{\bignorm}[1]{\Bigl \| #1 \Bigr \| #1}
\newcommand{\inner}[1]{\langle #1 \rangle}
\newcommand{\set}[1]{{ #1 }}


% -------------------------------------------------- %
% -------------------- SETUP ----------------------- %
% -------------------------------------------------- %
\title{\Huge{Lecture 9 - Moments and Exam Review}}
\author{\large{Mitch Harrison}}
\date{\today}   
\hfuzz=1pt
\begin{document}
\setlength{\parskip}{1\baselineskip}
\setlength{\parindent}{15pt}
\maketitle
\tableofcontents
\newpage


% -------------------------------------------------- %
% --------------------- BODY ----------------------- %
% -------------------------------------------------- %
\section{Review}

Recall that the \textbf{method of moments} (MoM) says that for data
$X_{1}, \cdots , X_{n} \overset{\mathrm{iid}}{\sim}f_{\theta}(x)$, we have the
expectations
\begin{align*}
    \mathbb{E}[X_{1}] &= \mu_{1}(\theta) \\
    \mathbb{E}[X_{2}] &= \mu_{2}(\theta)
\end{align*}
etc. These are the first, second, third, etc. \textbf{moments}. Recall that
\begin{align*}
    \mathbb{E}[X] &= \mu \\
    \mathbb{E}[X^{2}] &= Var(X) + \mathbb{E}(X)^{2} = \sigma^{2} + \mu^{2}.
\end{align*}

Let $M_{1}(X)$ be the first moment of $X$. We can estimate it with the sample
mean of the data. That is, $\mathbb{E}[X] = \mu_{1}(\theta)$. This is a
\textbf{law of large numbers} argument. We can then write,
\[
    \bar X^{2} = \frac{1}{n}\sum_{i=1}^{n}X_{i}^{2} \rightarrow \mathbb{E}[X^{2}]
    = \mu_{2}(\theta).
\]
This also works for all exponents/moment $p$. That is, for all $p$,
\[
    \bar X^{p} = \frac{1}{n}\sum_{i=1}^{n}X_{i}^{p} \rightarrow \mathbb{E}[X^{p}]
    = \mu_{p}(\theta).
\]
The idea between the \textit{method of moments} is that we can go from these
moments to parameters. Take the example from $\mu_{1}$ and $\mu_{2}$. If I have
a good estimate for $\mu_{2}$, we don't immediately have a good estimate for
$\mu_{1}$. However, we can rewrite
\begin{align*}
    \mu_{2} &= \sigma^{2} + \mu_{2} \\
    \mu_{2} - \mu_{1}^{2} &= \sigma^{2} 
\end{align*}
Thus, we can estimate $\mu_{1}$ with $M_{2}(X)$, and $\mu_{2}$ with $M_{1}(X)$.
Similarly, we can estimate $\mu$ with $M_{1}(X)$ and $\sigma^{2}$ with
$M_{2}(X) - M_{1}(X)^{2}$.

\subsubsection{MoM example}

A gamma distribution has two parameters, $\alpha$ and $\beta$. These parameters
are often non-trivial to find, but we can use the method of moments. Observe
that
\begin{align*}
    \mathbb{E}[X] = \mu_{1} &= \frac{\alpha}{\beta} \\
    \mathbb{E}[X^{2}] = \mu_{2} &= \frac{\alpha(\alpha+1)}{\beta^{2}}.
\end{align*}
We now need to find $\alpha$ and $\beta$ in terms of the moments.
\begin{align*}
    \beta &= \frac{\alpha}{\mu_{1}} \\
    \frac{\mu_{2}}{\mu_{1}} &= \frac{\alpha(\alpha+1)}{\alpha^{2}} \\
    &= 1 + \frac{1}{\alpha}
\end{align*}
Re-arranging, we find that
\[
    \alpha = \frac{1}{\frac{\mu^{2}}{\mu_{1}}-1} + \frac{1}{\frac{\mu_{2} -
        \mu_{1}^{2}}{\mu_{1}^{2}}}.
\]
And now that we have a value for $\alpha$, if we plug it back into
$\beta = \frac{\alpha}{\mu_{2}}$, we can simplify algebraically to
\[
    \beta = \frac{\mu_{1}}{\mu_{2} - \mu_{1}^{2}}.
\]

\pagebreak
\section{Exam Review}

Question types that are in scope:
\begin{enumerate}
    \item Definitions
    \item "State the theorem"
    \item Combinations of different concepts (Fisher information, CLT, etc.)
\end{enumerate}

\subsubsection{Example of "follow the steps"}
Consider the random variable $Y \sim Weibull(k, \lambda)$. The density is given
by
\[
p(y) = \frac{k}{\lambda}\left(\frac{y}{\lambda}\right)^{k-1}exp\{-y/\lambda)^{k}).
\]
We could be asked to
\begin{enumerate}
    \item Find the likelihood
    \item Find the log-likelihood
    \item Find the MLE.
\end{enumerate}
Alternatively, the density can be written as 
\[
    p(y) = \frac{ky^{k-1}}{b}exp\{-y^{k}/b\}
\]
for $k>0$ and $b>0$. We could be asked to explain in words how we get the 
same MLE result with this new density. The same question also asks us to find
the variance of $Y^{k}$. That is,
\[
Var(Y^{k}) = \mathbb{E}(Y^{2k}) - \mathbb{E}(Y^{k})^{2} = 2b^{2}-b^{2} = b^{2}.
\]
We then are asked to state Chebyshev's inequality. That is,
\[
\mathbb{P}(|X-\mu| \ge \epsilon) \le \frac{\sigma^{2}}{\epsilon^{2}}.
\]
Finally, we are told to argue that our estimator is consistent.

\subsection{What is on the text?}
This is a non-exhaustive list:
\begin{enumerate}
    \item \textbf{Definitions}: statistic, estimator, loss, bias, variance,
        mean squared error, finite efficiency, likelihood, log-likelihood,
        score function, Fisher information, sufficiency, method of moments,
        maximum likelihood estimation, consistency, asymptotic efficiency.
    \item \textbf{Results}: $\mathbb{E}[S(\theta)] = 0$, $\mathbb{E}[S(\theta)]
        ^{2}] = - \mathbb{E}[S'(\theta)]$, Cramer-Rao lower bound, factorization
        criterion, Rao-Blackwell, MLEs are consistent and asymptotically
        efficient (Fisher's approximation theorem).
\end{enumerate}

\begin{definition}
    We have two definitions of \textbf{sufficiency}. A statistic $T$ is
    \textit{sufficient} if,
    \begin{itemize}
        \item for a parameter $\theta$ in a model defined by the PDF
            $f(x|\theta)$ if $\mathbb{P}(X_{1}=x_{1}, \cdots ,X_{n}=x_{n}|
            T=t)$ does not depend on $\theta$ for any value of $\theta$.
        \item if and only if we can factor the likelihood as
            $h(\vec x)v(T(\vec x),\theta)$.
    \end{itemize}
\end{definition}

\subsubsection{Rao-Blackwell example}

Let $X_{1}$ and $X_{2}$ be independent $N(\theta,1)$ random variables. Our
friend was only going to use $X_{1}$ to learn about $\theta$, even though two
data points were collected. Which estimator ($X_{1}$ or $\bar X$) is more
\textit{efficient}?

We know that both $X_{1}$ and $\bar X$ are \textit{unbiased}. So if we show
that one of the two of them has lower variance, we could show that $\bar X$
is more efficient. Rao-Blackwell says that conditioning on a sufficient
statistic (e.g. $X_{1}|\bar X$) will improve the estimator $X_{1}$.

\begin{definition}
    Let $T$ be \textit{sufficient} for $\theta$, $\delta(X)$ an unbiased
    estimator of $\theta$, and define $\tilde{\delta}(X) = \mathbb{E}[
    \delta(X)|T]$. Then the MSE of $\tilde{\delta}$ is o greater than the MSE
    of $\delta$. This is \textbf{Rao-Blackwellization}.
\end{definition}

We want to find the expectation of $X_{1} | (X_{1}+X_{2})/2$. This might be
a challenging distribution to calculate, but luckily we need the expectation,
but \textit{not} the distribution.

\end{document}
